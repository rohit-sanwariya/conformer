{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa53d42",
   "metadata": {},
   "source": [
    "Group No : 28\n",
    "\n",
    "| BITS ID       | Name                         | Contribution |\n",
    "|---------------|------------------------------|-------------|\n",
    "| 2024aa05366   | JOKARE MAHESH SHIVANAND      | 100%        |\n",
    "| 2024aa05367   | ROHIT SANWARIYA              | 100%        |\n",
    "| 2024aa05369   | ANEESH K.V.                  | 100%        |\n",
    "| 2024aa05370   | JAISINGHANI ANJALI CHANDER VARSHA | 100%       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf4070d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fb9e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synthetic speech dataset...\n",
      "Dataset shape: (1000, 8000, 1)\n",
      "Sample labels: ['model model' 'test data' 'ai world' 'world world' 'model ai']\n"
     ]
    }
   ],
   "source": [
    "# Create synthetic speech dataset for demonstration\n",
    "def create_synthetic_speech_dataset(num_samples=1000, seq_length=8000):\n",
    "    \"\"\"Create synthetic audio data for speech recognition demo\"\"\"\n",
    "    print(\"Creating synthetic speech dataset...\")\n",
    "    \n",
    "    # Generate synthetic audio (mixture of sine waves)\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Simple vocabulary for demonstration\n",
    "    vocab = ['hello', 'world', 'ai', 'ml', 'test', 'data', 'model']\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Create synthetic audio (multiple sine waves)\n",
    "        t = np.linspace(0, 1, seq_length)\n",
    "        freq1 = 100 + np.random.randint(0, 200)\n",
    "        freq2 = 300 + np.random.randint(0, 200)\n",
    "        \n",
    "        audio = (0.5 * np.sin(2 * np.pi * freq1 * t) + \n",
    "                 0.3 * np.sin(2 * np.pi * freq2 * t) +\n",
    "                 0.1 * np.random.randn(seq_length))\n",
    "        \n",
    "        # Normalize\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "        \n",
    "        # Create simple text label\n",
    "        text_label = ' '.join(np.random.choice(vocab, size=2))\n",
    "        \n",
    "        X.append(audio)\n",
    "        y.append(text_label)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Generate dataset\n",
    "X, y = create_synthetic_speech_dataset(1000)\n",
    "X = np.expand_dims(X, -1)  # Add channel dimension\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Sample labels: {y[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39e9c078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed labels shape: (1000, 11)\n",
      "Vocabulary size: 15\n",
      "Max text length: 11\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "def create_text_processor(texts):\n",
    "    \"\"\"Create text tokenizer and preprocess labels\"\"\"\n",
    "    # Create character-level tokenizer\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "        char_level=True,\n",
    "        oov_token='<UNK>',\n",
    "        filters=''\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    sequences_padded = keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences, maxlen=max_len, padding='post'\n",
    "    )\n",
    "    \n",
    "    return sequences_padded, tokenizer, max_len\n",
    "\n",
    "# Process text labels\n",
    "y_processed, tokenizer, max_text_len = create_text_processor(y)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(f\"Processed labels shape: {y_processed.shape}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Max text length: {max_text_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f7db1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conformer components\n",
    "class ConformerBlock(layers.Layer):\n",
    "    \"\"\"Conformer block as described in the paper\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, conv_kernel_size=32, dropout_rate=0.1):\n",
    "        super(ConformerBlock, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        \n",
    "        # First feed-forward layer\n",
    "        self.ffn1 = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='swish'),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(embed_dim),\n",
    "            layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        \n",
    "        # Self-attention layer\n",
    "        self.self_attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embed_dim//num_heads,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.attention_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Convolution module\n",
    "        self.conv_module = keras.Sequential([\n",
    "            layers.LayerNormalization(epsilon=1e-6),\n",
    "            layers.Conv1D(embed_dim, 1),  # Pointwise conv\n",
    "            layers.Activation('swish'),\n",
    "            layers.DepthwiseConv1D(conv_kernel_size, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('swish'),\n",
    "            layers.Conv1D(embed_dim, 1),  # Pointwise conv\n",
    "            layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.conv_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Second feed-forward layer\n",
    "        self.ffn2 = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='swish'),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(embed_dim),\n",
    "            layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.final_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        # First FFN (half-step residual)\n",
    "        ffn1_out = self.ffn1(inputs, training=training)\n",
    "        x = inputs + 0.5 * ffn1_out\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_output = self.self_attention(x, x, training=training)\n",
    "        x = self.attention_norm(x + attn_output)\n",
    "        \n",
    "        # Convolution module\n",
    "        conv_output = self.conv_module(x, training=training)\n",
    "        x = self.conv_norm(x + conv_output)\n",
    "        \n",
    "        # Second FFN (half-step residual)\n",
    "        ffn2_out = self.ffn2(x, training=training)\n",
    "        x = x + 0.5 * ffn2_out\n",
    "        \n",
    "        return self.final_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83b29b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conformer Model Summary:\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " audio_input (InputLayer)       [(None, 8000, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 8000, 144)    576         ['audio_input[0][0]']            \n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " audio_input (InputLayer)       [(None, 8000, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 8000, 144)    576         ['audio_input[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 8000, 144)   576         ['conv1d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 8000, 144)    0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLambda  (3,)                0           ['activation[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  ()                  0           ['tf.compat.v1.shape[0][0]']     \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.range (TFOpLambda)          (8000,)              0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (8000, 144)          1152000     ['tf.range[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 8000, 144)   0           ['activation[0][0]',             \n",
      " da)                                                              'embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conformer_block (ConformerBloc  (None, 8000, 144)   464976      ['tf.__operators__.add[0][0]']   \n",
      " k)                                                                                               \n",
      "                                                                                                  \n",
      " conformer_block_1 (ConformerBl  (None, 8000, 144)   464976      ['conformer_block[0][0]']        \n",
      " ock)                                                                                             \n",
      "                                                                                                  \n",
      " conformer_block_2 (ConformerBl  (None, 8000, 144)   464976      ['conformer_block_1[0][0]']      \n",
      " ock)                                                                                             \n",
      "                                                                                                  \n",
      " conformer_block_3 (ConformerBl  (None, 8000, 144)   464976      ['conformer_block_2[0][0]']      \n",
      " ock)                                                                                             \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 8000, 144)    20880       ['conformer_block_3[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 8000, 144)    0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " ctc_output (Dense)             (None, 8000, 15)     2175        ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,036,111\n",
      "Trainable params: 3,034,671\n",
      "Non-trainable params: 1,440\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_conformer_model(input_shape, vocab_size, max_text_len, \n",
    "                         embed_dim=144, num_heads=4, ff_dim=576, \n",
    "                         num_blocks=4, conv_kernel_size=32):\n",
    "    \"\"\"Build complete Conformer model for speech recognition\"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    audio_input = layers.Input(shape=input_shape, name='audio_input')\n",
    "    \n",
    "    # Initial feature extraction\n",
    "    x = layers.Conv1D(embed_dim, 3, padding='same')(audio_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "    \n",
    "    # Add positional encoding\n",
    "    positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
    "    position_embedding = layers.Embedding(input_shape[0], embed_dim)(positions)\n",
    "    x = x + position_embedding\n",
    "    \n",
    "    # Stack Conformer blocks\n",
    "    for _ in range(num_blocks):\n",
    "        x = ConformerBlock(embed_dim, num_heads, ff_dim, conv_kernel_size)(x)\n",
    "    \n",
    "    # Output layers\n",
    "    x = layers.Dense(embed_dim, activation='swish')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # CTC output layer\n",
    "    output = layers.Dense(vocab_size, activation='softmax', name='ctc_output')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=audio_input, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "model = build_conformer_model(input_shape, vocab_size, max_text_len)\n",
    "\n",
    "print(\"Conformer Model Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "825d93e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC loss implementation\n",
    "def ctc_loss(y_true, y_pred):\n",
    "    \"\"\"Custom CTC loss function\"\"\"\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "    input_length = tf.shape(y_pred)[1]\n",
    "    label_length = tf.shape(y_true)[1]\n",
    "    \n",
    "    input_length = input_length * tf.ones(shape=(batch_size, 1), dtype='int32')\n",
    "    label_length = label_length * tf.ones(shape=(batch_size, 1), dtype='int32')\n",
    "    \n",
    "    return keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "\n",
    "# Custom accuracy metric for CTC\n",
    "# Fixed CTC accuracy metric\n",
    "class CTCAccuracy(keras.metrics.Metric):\n",
    "    def __init__(self, name='ctc_accuracy', **kwargs):\n",
    "        super(CTCAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.correct_count = self.add_weight(name='correct', initializer='zeros')\n",
    "        self.total_count = self.add_weight(name='total', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Decode predictions\n",
    "        input_length = tf.shape(y_pred)[1]\n",
    "        input_length = input_length * tf.ones(shape=(tf.shape(y_pred)[0],), dtype='int32')\n",
    "        \n",
    "        decoded, _ = tf.nn.ctc_greedy_decoder(\n",
    "            tf.transpose(y_pred, perm=[1, 0, 2]),\n",
    "            input_length\n",
    "        )\n",
    "        \n",
    "        # Convert to dense and ensure same type\n",
    "        decoded = tf.sparse.to_dense(decoded[0], default_value=-1)\n",
    "        decoded = tf.cast(decoded, tf.int32)  # Convert to int32\n",
    "        \n",
    "        # Ensure y_true is also int32\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        \n",
    "        # Calculate accuracy - handle variable length sequences\n",
    "        correct = tf.reduce_all(\n",
    "            tf.equal(decoded[:, :tf.shape(y_true)[1]], y_true), \n",
    "            axis=1\n",
    "        )\n",
    "        correct = tf.cast(correct, tf.float32)\n",
    "        \n",
    "        self.correct_count.assign_add(tf.reduce_sum(correct))\n",
    "        self.total_count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "    \n",
    "    def result(self):\n",
    "        return self.correct_count / self.total_count\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.correct_count.assign(0.)\n",
    "        self.total_count.assign(0.)\n",
    "\n",
    "# Alternative simpler CTC accuracy metric (if above still has issues)\n",
    "class SimpleCTCAccuracy(keras.metrics.Metric):\n",
    "    def __init__(self, name='ctc_accuracy', **kwargs):\n",
    "        super(SimpleCTCAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.correct_count = self.add_weight(name='correct', initializer='zeros')\n",
    "        self.total_count = self.add_weight(name='total', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Simple implementation - just track if we can decode something\n",
    "        input_length = tf.shape(y_pred)[1]\n",
    "        input_length = input_length * tf.ones(shape=(tf.shape(y_pred)[0],), dtype='int32')\n",
    "        \n",
    "        decoded, _ = tf.nn.ctc_greedy_decoder(\n",
    "            tf.transpose(y_pred, perm=[1, 0, 2]),\n",
    "            input_length\n",
    "        )\n",
    "        \n",
    "        # Just check if we decoded anything (simplified)\n",
    "        decoded_dense = tf.sparse.to_dense(decoded[0], default_value=-1)\n",
    "        has_prediction = tf.cast(tf.reduce_any(decoded_dense != -1, axis=1), tf.float32)\n",
    "        \n",
    "        self.correct_count.assign_add(tf.reduce_sum(has_prediction))\n",
    "        self.total_count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "    \n",
    "    def result(self):\n",
    "        return self.correct_count / self.total_count if self.total_count > 0 else 0.0\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.correct_count.assign(0.)\n",
    "        self.total_count.assign(0.)\n",
    "    def __init__(self, name='ctc_accuracy', **kwargs):\n",
    "        super(CTCAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.correct_count = self.add_weight(name='correct', initializer='zeros')\n",
    "        self.total_count = self.add_weight(name='total', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Decode predictions\n",
    "        input_length = tf.shape(y_pred)[1]\n",
    "        input_length = input_length * tf.ones(shape=(tf.shape(y_pred)[0],), dtype='int32')\n",
    "        \n",
    "        decoded, _ = tf.nn.ctc_greedy_decoder(\n",
    "            tf.transpose(y_pred, perm=[1, 0, 2]),\n",
    "            input_length\n",
    "        )\n",
    "        \n",
    "        # Compare with true labels\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        decoded = tf.sparse.to_dense(decoded[0], default_value=-1)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = tf.reduce_all(tf.equal(decoded, y_true), axis=1)\n",
    "        correct = tf.cast(correct, tf.float32)\n",
    "        \n",
    "        self.correct_count.assign_add(tf.reduce_sum(correct))\n",
    "        self.total_count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "    \n",
    "    def result(self):\n",
    "        return self.correct_count / self.total_count\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.correct_count.assign(0.)\n",
    "        self.total_count.assign(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66714267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (800, 8000, 1), (800, 11)\n",
      "Validation data: (200, 8000, 1), (200, 11)\n",
      "Training Conformer model...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/conformer_block/multi_head_attention/einsum/Einsum' defined at (most recent call last):\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_11616\\1885145611.py\", line 18, in <module>\n      history = model.fit(\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_11616\\3308336230.py\", line 54, in call\n      attn_output = self.self_attention(x, x, training=training)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 596, in call\n      attention_output, attention_scores = self._compute_attention(\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 525, in _compute_attention\n      attention_scores = tf.einsum(self._dot_product_equation, key, query)\nNode: 'model/conformer_block/multi_head_attention/einsum/Einsum'\nOOM when allocating tensor with shape[8,4,8000,8000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node model/conformer_block/multi_head_attention/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_17872]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Conformer model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/conformer_block/multi_head_attention/einsum/Einsum' defined at (most recent call last):\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_11616\\1885145611.py\", line 18, in <module>\n      history = model.fit(\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_11616\\3308336230.py\", line 54, in call\n      attn_output = self.self_attention(x, x, training=training)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 596, in call\n      attention_output, attention_scores = self._compute_attention(\n    File \"c:\\Users\\rohit\\.conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 525, in _compute_attention\n      attention_scores = tf.einsum(self._dot_product_equation, key, query)\nNode: 'model/conformer_block/multi_head_attention/einsum/Einsum'\nOOM when allocating tensor with shape[8,4,8000,8000] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node model/conformer_block/multi_head_attention/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_17872]"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=ctc_loss,\n",
    "    metrics=[CTCAccuracy()]\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_processed, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training data: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation data: {X_val.shape}, {y_val.shape}\")\n",
    "\n",
    "# Train model\n",
    "print(\"Training Conformer model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=8,\n",
    "    epochs=15,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ddac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"Evaluating model...\")\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Validation CTC Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['ctc_accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_ctc_accuracy'], label='Validation Accuracy')\n",
    "plt.title('CTC Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('CTC Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('conformer_training.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict_speech(audio_sample, model, tokenizer):\n",
    "    \"\"\"Predict text from audio sample\"\"\"\n",
    "    # Add batch dimension\n",
    "    audio_sample = np.expand_dims(audio_sample, axis=0)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(audio_sample, verbose=0)\n",
    "    \n",
    "    # Decode using greedy decoder\n",
    "    input_length = np.ones(predictions.shape[0]) * predictions.shape[1]\n",
    "    decoded, _ = tf.nn.ctc_greedy_decoder(\n",
    "        tf.transpose(predictions, perm=[1, 0, 2]),\n",
    "        input_length\n",
    "    )\n",
    "    \n",
    "    # Convert to text\n",
    "    decoded = tf.sparse.to_dense(decoded[0]).numpy()\n",
    "    predicted_text = tokenizer.sequences_to_texts(decoded)[0]\n",
    "    \n",
    "    return predicted_text\n",
    "\n",
    "# Test prediction\n",
    "test_sample = X_val[0]\n",
    "true_text = tokenizer.sequences_to_texts([y_val[0]])[0]\n",
    "predicted_text = predict_speech(test_sample, model, tokenizer)\n",
    "\n",
    "print(f\"True text: {true_text}\")\n",
    "print(f\"Predicted text: {predicted_text}\")\n",
    "\n",
    "# Save model\n",
    "model.save('conformer_speech_model.h5')\n",
    "print(\"Model saved as 'conformer_speech_model.h5'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
